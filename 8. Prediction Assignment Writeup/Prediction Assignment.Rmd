---
title: "Prediction Assignment Writeup"
author: "Milton Candela"
date: "1/14/2021"
output:
  html_document:
    fig_width: 15
    fig_height: 10
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(randomForest)
library(caret)
library(rattle)
library(rpart)
library(lattice)
set.seed(1000)
```

# Introduction

This project use data from *groupware*, in their investigation of Human Activity Recognition <http://groupware.les.inf.puc-rio.br/har>, which has the purpose of predicting how well is an exercise made, the factor variable **classe** tells how well the exercise (Unilateral Dumbbell Biceps Curl) is done: according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

the data was collected via an ambient sensing approach (by using Microsoft Kinect) and had a lot of features in it, but for the purpose of this investigation, only features corresponding to the acceleration of the person will be used. 

# Loading the libraries

Five packages will be needed in order to do the analysis:

* **dplyr**: Used to manipulate the training data set
* **randomForest**: Used to create Model 3 and Model 4
* **caret**: Used to model fitting
* **rattle**: Used to visualize **rpart** type of objects
* **rpart**: Used to create Model 1 and Model 2
* **lattice**: used to create the heatmap of the final model

It is important to set the seed to **1000**, so that **randomForest** provides consistent, reproducible results.

```{r libraries, results= FALSE, error = FALSE, warning = FALSE, eval = FALSE}
library(dplyr)
library(randomForest)
library(caret)
library(rattle)
library(rpart)
library(lattice)
set.seed(1000)
```

# Loading the data

Data sets used:

* Training dataset: <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv>
* Testing dataset: <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>

```{r getting_data}
if(!file.exists('training.csv')){
       download.file(url = 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv',
                     destfile = 'training.csv', method = 'curl')
}
training <- read.csv('training.csv')

if(!file.exists('testing.csv')){
       download.file(url = 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv',
                     destfile = 'testing.csv', method = 'curl')
}
testing <- read.csv('testing.csv')
```

Both data sets have 160 columns, fortunately we will not be using everyone of this features:

```{r}
print(data.frame(train_col = length(colnames(training)), test_col = length(colnames(testing))))
```

Now let's see how which of the features correspond to the acceleration of the subject

```{r}
print(grep('accel',colnames(training), value = TRUE))
```

From these features, two main datasets will be created:

* **training_acc_xyz** which contains the x, y, z positions
* **training_acc_Tot** which contains the total acceleration, the variance and total variance will not be included because of the big number of NAs that the data contains.

```{r}
for (feature in grep('var_accel',colnames(training), value = TRUE)) {
       print(paste('Percent of NA from the feature', feature, ': ',
                   (sum(is.na(training[,feature])))/(length(training[,feature])) * 100, '%'))
}
```

# Datasets creation

We will be creating both datasets in this section

#### First dataset: *training_acc_xyz*

In order to create this data set, we need to subset he data and only obtain the acceleration features, and then subsetting again so that only features that contain the x, y and z parameters from each of the movements.

```{r datasets_best}
training_accel <- training[,c(1, 2, grep('accel',colnames(training)), 160)]
training_accel <- mutate(training_accel, classe = as.factor(classe))

training_acc_xyz <- training_accel[c(length(training_accel[1,]), grep('accel', colnames(training_accel)))]
training_acc_xyz <- training_acc_xyz[,-grep("total|var_", colnames(training_acc_xyz))]
str(training_acc_xyz)
```

#### Second data set: *training_acc_Tot*

The same procedure is done to create the total part of the acceleration, this data was ordered so that the class is the first column of the data.

```{r datsets_tot}
training_acc_Tot <- training_accel[,c(grep('total_accel', colnames(training_accel)),
                                           length(colnames(training_accel)))]
training_acc_Tot <- training_acc_Tot[,length(colnames(training_acc_Tot)):1]
training_acc_Tot <- subset(training_acc_Tot, select = -5)
str(training_acc_Tot)
```

# Models creation

Predictive models will be created using their respective R functions, as an example: **randomForest()** and **rpart()**.

#### Model 1

The first model corresponds to a tree created by the **rpart** function, which displays a big decision tree but with enough zoom the decisions can be visualized, it is worth say that the **fancyRpartPlot** function corresponds to the **rattle** package, which displays a good-looking dendogram. This model use the total acceleration (as it can be seen inside the **rpart** funcion *data = training_acc_Tot*)

```{r model1}
model1 <- rpart(classe ~ ., data = training_acc_Tot, na.action = na.omit)
fancyRpartPlot(model1, main = 'Rpart visualization of Model 1', sub = '')
mod1Pred <- predict(model1, testing, type = 'class')
```

#### Model 2

For the second model, the same procedure will be used but with the *training_acc_xyz* dataset, which takes into account the raw positions.

```{r model2}
model2 <- rpart(classe ~ ., data = training_acc_xyz, na.action = na.omit)
fancyRpartPlot(model2, main = 'Rpart visualization of Model 2', sub = '')
mod2Pred <- predict(model2, testing, type = 'class')
```

#### Model 3

And for the third model, **randomForest** will be used, using data from *training_acc_Tot* dataset. It can be observed that the OOB estimate of error rate is 30.34%, which is really high, and there is a lot of error in the confusion matrix.

```{r model3}
model3 <- randomForest(classe ~ ., data = training_acc_Tot)
print(model3)
mod3Pred <- predict(model3, testing)
```

#### Model 4

While the fourth model will use the same method as Model 3 but using the data from *training_acc_xyz* dataset. This model presents an OOB estimate of error rate of 4.23%, which depicts a stronger model with more confidence in its predictions.

```{r model4}
model4 <- randomForest(classe ~ ., data = training_acc_xyz)
print(model4)
mod4Pred <- predict(model4, testing)
```

# Accuracy

We then test the accuracy from the predictive models and their predictions, we will first set the predictions on a data frame in order to visualice if their first elements appear to be similar.

```{r models}
real <- as.factor(c('B', 'A', 'B', 'A', 'A',
                    'E', 'D', 'B', 'A', 'A',
                    'B', 'C', 'B', 'A', 'E',
                    'E', 'A', 'B', 'B', 'B'))
predDF <- data.frame(mod1Pred, mod2Pred, mod3Pred, mod4Pred, real)
head(predDF)
```

There is not much of a similarity between the models, there are some similar classes in which they all seem to agree (mainly on guessing the A class), which means that these predictive models can be used for binary classification (if a person is doing the exercise properly or no), we shall then test their accuracy in regard of the real class.

```{r models_acc}
for (pred in predDF[,1:4]) {
       print(confusionMatrix(real, pred)$overall)
}
```

We can see that the first two models perform poorly, AUC < 0.50 is similar to flipping a coin so those models are not relevant. While the last two models got 95% accuracy, which proves that **randomForest** method functions well on the prediction of these behaviors. Although accuracy was 95% for model 3 and 4, model 4 predicted the < 5% error rate while model 3 predicted > 30%. So combining predictors is not a great idea in this case, because only model 4 was accurate enough with the training set, as well as the test set, including the final validation classes, let's further analyze the final model (model 4).

```{r final}
modelFinal <- model4
```

# Analysis

We will first take a look at the confusion matrix.

```{r conf}
print(modelFinal$confusion)
confm <- modelFinal$confusion[,-6]
```

As it can be seen, values are most concentrated across the diagonal, which means that the accuracy is really high because most of the predicted values were the actual values. Now let's use the **levelplot** function from the **lattice** package in order to plot the heatmap of this matrix.

```{r heatmap}
sum_row <- apply(confm, FUN = sum, MARGIN = 2)
conf_centage <- sweep(confm, 2, sum_row, FUN = '/')
levelplot(conf_centage, col.regions = terrain.colors(100), xlab = 'Actual Class', ylab = 'Predicted Class',
          main = 'Heatmap made by the confusion matrix of the final predictive model')
```

Finally, we will take a look at the plot that the *randomForest* object generates, this will be a graph that represents the quantity of error with respect to the number of trees generates in each iteration. 

```{r treeplot}
plot(modelFinal, main = 'Final Model Plot')
```

As it can be seen, the error drops below 10% when, approximately, 50 tress are generated, maybe that is the reason why the *rpart* method could not deliver a good accuracy rate as the predictive models with *randomForest* did.